{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d99ac0",
   "metadata": {},
   "source": [
    "importing the relevant functions to use during the process of building the Multilayered Artificial Neural Network.\n",
    "Functions imported are used for mathematical operations, loading of data, data preprocessing, and finally plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e162260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "import os\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3690a",
   "metadata": {},
   "source": [
    "Functions for data loading, and preprocessing including data splitting and standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36d3013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(filename):\n",
    "    \"\"\"\n",
    "    This function is used in loading the data as dataframes using pandas.\n",
    "    Args:\n",
    "        filename (str): A string which represents the name of the file to be loaded.\n",
    "    Returns:\n",
    "        A dataframe which  represents the dataframe returned after loading the data.\n",
    "    \"\"\"\n",
    "    # set header as None to avoid setting relevant columns as headers.\n",
    "    loaded_data = pd.read_csv(filename, header = None)\n",
    "    \n",
    "    return loaded_data\n",
    "\n",
    "def get_label_features(data_frame, label_index):\n",
    "    \"\"\"\n",
    "    This function returns the label and feature of the dataset \n",
    "    Args:\n",
    "        data_frame - A dataframe obtained after loading the data using pandas.\n",
    "        label_index - The index which represents the position of the label\n",
    "    Returns:\n",
    "        X, Y (two dataframes) which represents the Features as well as the Labels.\n",
    "    \"\"\"\n",
    "    Y = data_frame[label_index]\n",
    "    X = data_frame.drop([label_index], axis = 1)\n",
    "    return X, Y\n",
    "    \n",
    "def convert_to_numpy(pd_value):\n",
    "    \"\"\"\n",
    "    This function converts the panda data into a numpy array.\n",
    "    Args: \n",
    "        pd_value - A dataframe value to be converted to an numpy array\n",
    "    Returns:\n",
    "        A numpy array.\n",
    "    \"\"\"\n",
    "    return np.array(pd_value)\n",
    "\n",
    "\n",
    "def encode_label(encoder, label):\n",
    "    \"\"\"\n",
    "    This function encodes a label with categorical values into numerical values\n",
    "    Args:\n",
    "        encoder - This represents the encoder function to be used.\n",
    "        label - This represents the categorical dataframe to encode.\n",
    "    \n",
    "    Returns:\n",
    "        An encoded value with numerical values arranged in ascending order.\n",
    "    \"\"\"\n",
    "    return encoder.fit_transform(label)\n",
    "\n",
    "\n",
    "def expand_classes(label, num_classes):\n",
    "    \"\"\"\n",
    "    This function breaks a label into mutiple columns that represents the number of classes contained in the function.\n",
    "    Args:\n",
    "        label(numpy): A numpy array that represents the label\n",
    "        num_classes(int): An integer that represents the nmumber of classes.\n",
    "    \"\"\"\n",
    "    # create an identity matrix with num_classes rows and columns\n",
    "    get_identity = np.eye(num_classes)\n",
    "    \n",
    "    return (get_identity)[label]\n",
    "    \n",
    "def standardize_features(scaler, features):\n",
    "    \"\"\"\n",
    "    This function helps to standardize a feature within a given range to reduce variance.\n",
    "    Args:\n",
    "        scaler - This represents the scaler to be applied to the dataset\n",
    "        features (numpy) -  numpy array that represents the data to apply the scaler to.\n",
    "    \n",
    "    \"\"\"\n",
    "    return scaler.transform(features)\n",
    "\n",
    "def get_shape(value):\n",
    "    \"\"\"\n",
    "    This function gets the shape of the value\n",
    "    Args: \n",
    "        value: A numpy array or a dataframe whose shape we want to get.\n",
    "    Returns:\n",
    "        returns a tuple which represents the shape of the value\n",
    "    \"\"\"\n",
    "    \n",
    "    return value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90feac0f",
   "metadata": {},
   "source": [
    "functions for activation functions and its derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4fab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_activ_func(activ_func, A):\n",
    "    \"\"\"\n",
    "    This function helps to anipulate an Array using it's activation function\n",
    "    Args:\n",
    "        activ_func (str) - A string which represents the activation function\n",
    "        A(numpy) - A numpy array which represents the array we wish to apply the activation function equation to\n",
    "    \n",
    "    Returns:\n",
    "        None if Activation function is invalid or a numpy array that represents the computed value. \n",
    "    source: https://www.v7labs.com/blog/neural-networks-activation-functions\n",
    "    \"\"\"\n",
    "    \n",
    "    ## if the activation functin is sigmoid\n",
    "    if activ_func.lower() == \"sigmoid\":\n",
    "        return 1 / (1 + np.exp(-A))\n",
    "    # if the activation function is relu\n",
    "    elif activ_func.lower() == \"relu\":\n",
    "        return np.maximum(0, A)\n",
    "    # if the activation function is tanh\n",
    "    elif activ_func.lower() == \"tanh\":\n",
    "        return np.tanh(A)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def compute_derivat(activ_func, A):\n",
    "    \"\"\"\n",
    "    This function computes the derivative of a numpy array based on it's activation function\n",
    "    Args:\n",
    "        activ_func (str) - A string which represents the activation function\n",
    "        A(numpy) - A numpy array which represents the array we wish to apply the activation function equation to\n",
    "    Returns:\n",
    "        None if Activation function is invalid or a numpy array that represents the computed value. \n",
    "    source: https://yashgarg1232.medium.com/derivative-of-neural-activation-function-64e9e825b67\n",
    "    \"\"\"\n",
    "    # if the activatin function is sigmoid\n",
    "    if activ_func.lower() == \"sigmoid\":\n",
    "        Z = compute_activ_func(activ_func, A)\n",
    "        return np.multiply(Z, 1 - Z)\n",
    "    # if the activation is relu\n",
    "    elif activ_func.lower() == \"relu\":\n",
    "        return np.where(A <= 0, 0, 1)\n",
    "    # if the activation function is tanh\n",
    "    elif activ_func.lower() == \"tanh\":\n",
    "        Z = compute_activ_func(activ_func, A)\n",
    "        return 1 - np.square(Z)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccec084",
   "metadata": {},
   "source": [
    "functions for cost functions and optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed68f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(cost_fn, y_true, y_pred) -> float:\n",
    "    \"\"\"\n",
    "    This function computes the cost function of the model.\n",
    "    Args:\n",
    "        cost_fn(str) - A string whch represents the cost function type\n",
    "        Y_true(numpy) - A numpy array which represents the actual ouput\n",
    "        Y_pred(numpy) - A numpy array which represents the predicted output.\n",
    "    \n",
    "    Returns:\n",
    "        A float which represents the cost.\n",
    "    \"\"\"\n",
    "    size = get_shape(y_true)[1]\n",
    "    \n",
    "    # if the cost function is binary_cross_entropy\n",
    "    if cost_fn.lower() == \"binary_cross_entropy\":\n",
    "        logprobs = (np.multiply(y_true, np.log(y_pred)) + np.multiply((1-y_true),np.log(1-y_pred))) / size\n",
    "        cost = -np.sum(logprobs)\n",
    "        return float(np.squeeze(cost)) \n",
    "    elif cost_fn.lower() == \"mse\":\n",
    "        get_loss = np.square(y_true - y_pred) / size\n",
    "        return np.sum(get_loss)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3932d23b",
   "metadata": {},
   "source": [
    "functions for plotting the class distribution, as well as model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29070883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_score(pred, actual):\n",
    "    \"\"\"\n",
    "    This function computes the accuracy score of the model\n",
    "    Args: \n",
    "        pred - represents the predicted value\n",
    "        actual- represents the actual values\n",
    "    Returns:\n",
    "        A float which represents the score.\n",
    "    \"\"\"\n",
    "    sample_size = actual.shape[1]\n",
    "    Tp = np.dot(pred, actual.T)[0][0]\n",
    "    Tn = np.dot(1 - pred, 1 - actual.T)[0][0]\n",
    "    \n",
    "    accuracy = float(Tp + Tn) / float(sample_size)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def confusion_matrix(pred, Y):\n",
    "    return np.dot(pred, Y.T)\n",
    "\n",
    "def confuse_plot(data, title):\n",
    "    \"\"\"\n",
    "    This function displays the confusion matrix of the prediction derived from the model.\n",
    "    Args:\n",
    "        data(numpy) - An array which represents the computed confusion matrix\n",
    "        title(string) - A vlaue which represent the title of the plot.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (10,8))\n",
    "    # were 'cmap' is used to set the accent colour\n",
    "    sns.heatmap(data, annot=True, cmap= 'flare',  fmt='d', cbar=True)\n",
    "    plt.xlabel('Predicted_Label')\n",
    "    plt.ylabel('Truth_Label')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc29657",
   "metadata": {},
   "source": [
    "function for building the multilayered neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f5e403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_size(N):\n",
    "    \"\"\"\n",
    "    This function returns the number of layers in the network \n",
    "    \"\"\"\n",
    "    return len(N)\n",
    "\n",
    "\n",
    "def get_nodes(value):\n",
    "    \"\"\"\n",
    "    This function returns the number of nodes in a given array.\n",
    "    Args:\n",
    "        value(array): 2 dimensional array which represents the layer which we want to get it's node.\n",
    "    Returns:\n",
    "        An integer which represents the nodes\n",
    "    \"\"\"  \n",
    "    return value.shape\n",
    "\n",
    "def multilayer_parameters_initialize(dimension) -> dict:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dimension (list) -> Represent a list which contains the nodes for each layer \n",
    "    Returns:\n",
    "        A dictionary which represents the parameters of the model.               \n",
    "    \"\"\"\n",
    "    \n",
    "    # for reproducible of weights, we set a seed.\n",
    "    np.random.seed(50)\n",
    "    params = {}\n",
    "    \n",
    "    # get the number of layers in the layer.\n",
    "    N = len(dimension)\n",
    "    \n",
    "    #  the least we expect is a 2 Layered Neural network.\n",
    "    if N < 3:\n",
    "        return None\n",
    "\n",
    "    for layer in range(1, N):\n",
    "        params['W' + str(layer)] = np.random.randn(dimension[layer], dimension[layer-1]) * 0.01\n",
    "        params['b' + str(layer)] = np.zeros((dimension[layer], 1))\n",
    "        \n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def compute_forward_prop(X, params, activ_fns, layer_size) -> dict:\n",
    "    \"\"\"\n",
    "    This function computes the forward propagation of the model with initialized or updated set of parameters and \n",
    "    activation function.\n",
    "\n",
    "    Args:\n",
    "        X(numpy): An array which represents the input value to the layer\n",
    "        params(dict) - A dictionary that represents the parameter of the models\n",
    "        activ_fn(list): A list of string which represents the activation function.\n",
    "    Returns:\n",
    "        A dictionary represents the output of the model as well as the activation function passed.\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    for i in range(1, layer_size):\n",
    "        # store the weights and bias\n",
    "        W = params[\"W\"+str(i)]\n",
    "        b = params[\"b\"+str(i)]\n",
    "        activ_fn = activ_fns[i - 1]\n",
    "        # Here we are computing for Z1, A1, Z2,  A2, and the rest\n",
    "        if i == 1:\n",
    "            cache[\"act\" + str(i)] = activ_fn\n",
    "            cache[\"Z\" + str(i)] = np.dot(W, X) + b\n",
    "            cache[\"A\" + str(i)] = compute_activ_func(activ_fn, cache[\"Z\" + str(i)])\n",
    "        else:\n",
    "            cache[\"act\" + str(i)] = activ_fn\n",
    "            cache[\"Z\" + str(i)] = np.dot(W, cache[\"A\" + str(i-1)]) + b\n",
    "            cache[\"A\" + str(i)] = compute_activ_func(activ_fn, cache[\"Z\" + str(i)])\n",
    "                \n",
    "    return cache\n",
    "\n",
    "def compute_backward_prop(cache, layer_size, Y, params, X) -> dict:\n",
    "    \"\"\"\n",
    "    This function computes the gradient of the model which would be used later to update the parameters of the model.\n",
    "    Args:\n",
    "        dA (Array): An array which would be feed into the function at first.\n",
    "        cache(dict): A dictionary containing outputs from forward propagation\n",
    "        layer_size(int): An integer that represents the size of the layer used in the model.\n",
    "        Y(array): An array which represents the actual output of the model.\n",
    "        params (dict) - A dictionary which represents the weight and bias of the model\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary which stores the gradients\n",
    "    \"\"\"\n",
    "    # don't count the input layer.\n",
    "    layer = layer_size - 1\n",
    "    # get the number or records\n",
    "    size = Y.shape[1]\n",
    "    # initialize an empty dictionary to hold the gradients.\n",
    "    grad = {}\n",
    "    AL = cache[\"A\" + str(layer)]\n",
    "    \n",
    "    # start the backward propagation\n",
    "    #     dA = -(np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "    dA = AL - Y\n",
    "    # dW2 = W\n",
    "    act = cache[\"act\" + str(layer)]\n",
    "    dZ = np.multiply(dA, compute_derivat(act, cache[\"A\" + str(layer)]))\n",
    "    \n",
    "    # get both dWL and dbL\n",
    "    grad[\"dW\" + str(layer)] = (np.dot(dZ, cache[\"A\" + str(layer - 1)].T)) / size\n",
    "    grad[\"db\" + str(layer)] = np.sum(dZ, keepdims = True, axis = 1) / size\n",
    "    \n",
    "    \n",
    "    \n",
    "    # apply derivative for each activation function accross each layers going backward.\n",
    "    for i in range(layer - 1, 0, -1):\n",
    "        if i == 1:\n",
    "            cache[\"A\" + str(i-1)] = X\n",
    "        # get the weight, activaton function, \n",
    "        act = cache[\"act\" + str(i)]\n",
    "        W = params[\"W\" + str(i+1)]\n",
    "        dX = np.dot(W.T, dA)\n",
    "        # compute the derivative for the activation function.\n",
    "        dA = np.multiply(dX, compute_derivat(act, cache[\"A\" + str(i)]))\n",
    "        grad[\"dW\" + str(i)] = (np.dot(dA, cache[\"A\" + str(i - 1)].T)) / size\n",
    "        grad[\"db\" + str(i)] = np.sum(dA, keepdims = True, axis = 1) / size\n",
    "    \n",
    "    return grad\n",
    "\n",
    "\n",
    "def reset_params(params, grads, learning_rate, layer_size):\n",
    "    \"\"\"\n",
    "    This function update parameters using the gradient descent algorithm\n",
    "    \n",
    "    Args:\n",
    "        params (dict) - A dict which represents the initial parameters \n",
    "        grads (dict) - A dict which represents the gradients from the backward propagation.\n",
    "        layer_size(int): An integer that represents the size of the layer.\n",
    "        learning_rate(float): A float which represents the value use in update the weight and bias.\n",
    "    \n",
    "    Returns:\n",
    "        A dict which represents the updated parameter.\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    N = layer_size\n",
    "\n",
    "    for l in range(N-1):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\"+str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Build_Multi_ANN(X, Y, activation_functions, ann_size, cost_fn, ann_layer, learning_rate, Epochs):\n",
    "    \"\"\"\n",
    "    This function builds the Multi-layer ANN, to get a better value of weight and bias to use.\n",
    "    Args:\n",
    "        X(Array) - This represents the input to the model\n",
    "        Y(array) - This represents the actual output.\n",
    "        activation_functions (list) - This represents the set of activation function used.\n",
    "        ann_size (int) - This represents the size of the ANN layer\n",
    "        cost_fn(str) - This represents the cost function to compute the loss\n",
    "        ann_layer(list) - This represents the layers with their nodes.\n",
    "        learning_rate(float): represents how fast we want the model to descent\n",
    "        Epochs (int): Representts the number of times we want to train the model.\n",
    "    \n",
    "    Returns:\n",
    "        Two values representing the losses and the updated parameter\n",
    "        \n",
    "    \"\"\"\n",
    "    cost = []\n",
    "    # first initialize the parameters.\n",
    "    parameters = multilayer_parameters_initialize(ann_layer)\n",
    "    for i in range(Epochs):\n",
    "        # perform forward propagation to get the output from each layers, and also a final output\n",
    "        forward_cache = compute_forward_prop(X, parameters, activation_functions, ann_size)\n",
    "        # extract the fina output as it would be used in computing the cost.\n",
    "        final_output = forward_cache[\"A\"+str(ann_size -1)]\n",
    "        # compute the cost\n",
    "        get_cost = cost_function(cost_fn, Y, final_output)\n",
    "        # perform backward propagation to get gradient that would be used in updating the parameters to minimize errors.\n",
    "        get_grad = compute_backward_prop(forward_cache, ann_size, Y, parameters, X)\n",
    "        # update the parameters with the computed gradients and learning rate.\n",
    "        parameters = reset_params(parameters, get_grad, learning_rate, ann_size)\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f'cost after {i} epochs is {get_cost}')\n",
    "#             cost.append[get_cost]\n",
    "            \n",
    "    return cost, parameters\n",
    "\n",
    "def model_prediction(X, params, act_fns, ann_size):\n",
    "    \"\"\"\n",
    "    This function performs the model prediction\n",
    "    Args:\n",
    "        X represents the input\n",
    "        params represents the ann updated parameters\n",
    "        act_fns represents all activations used\n",
    "        ann_size represents the size\n",
    "    Returns:\n",
    "        An array which represents the predicted output\n",
    "    \"\"\"\n",
    "    get_cache = compute_forward_prop(X, params, act_fns, ann_size)\n",
    "    predicted_output = get_cache[\"A\" + str(ann_size - 1)]\n",
    "    predict = (predicted_output > 0.5)\n",
    "    \n",
    "    return predict\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7096f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the working directory of the system.\n",
    "current_dir = os.getcwd()\n",
    "DATASET = \"wdbc.data\"\n",
    "# combine the dataset with the file path.\n",
    "set_file = os.path.join(current_dir, DATASET)\n",
    "# load the dataset.\n",
    "load_data = input_data(set_file)\n",
    "# show the data\n",
    "display(load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a2e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The shape of the data after loading it is {get_shape(load_data)}')\n",
    "# The first column is not relevant, and might result to poor generalization of the model\n",
    "# according to the data description, it refers to the image id. Hence drop it.\n",
    "new_data = load_data.drop([0], axis = 1)\n",
    "print(f'The shape of the data after dropping the first column is {get_shape(new_data)}')\n",
    "display(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ac397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the input and labels\n",
    "# The label represents the column with categorical values M and B i.e column 1.\n",
    "feature, label = get_label_features(new_data, 1)\n",
    "print(\"Showing the features\")\n",
    "print()\n",
    "display(feature)\n",
    "print()\n",
    "print(\"Showing the labels\")\n",
    "print()\n",
    "display(label)\n",
    "print()\n",
    "print(f'Shape for feature is {get_shape(feature)} and Shape for label is {get_shape(label)}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of classes in the label, since we have \n",
    "NUM_CLASSES = 2\n",
    "# convert to numpy arrays.\n",
    "converted_features = convert_to_numpy(feature)\n",
    "converted_labels = convert_to_numpy(label)\n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "new_label = encode_label(label_encoder, converted_labels)\n",
    "print(f'label after conversion is\\n\\n {new_label}')\n",
    "print()\n",
    "print(f'Shape after encoding the data is {get_shape(new_label)}')\n",
    "print()\n",
    "# breaking the label to allow each class occupy a distinct column\n",
    "new_label = expand_classes(new_label, NUM_CLASSES)\n",
    "print(f'Shape after expanding the data is {get_shape(new_label)}')\n",
    "print()\n",
    "print(f'label after expanding is\\n\\n {new_label}')\n",
    "print()\n",
    "print(\"1 represents Malignant and 0 represents Benigh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92852c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the dataset, precisely the features.\n",
    "scaler = StandardScaler().fit(feature)\n",
    "standardize_feature = standardize_features(scaler, feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transpose both the input and outputs\n",
    "new_input = standardize_feature.T\n",
    "new_output = new_label.T\n",
    "\n",
    "print(f'Shape of label after transposing the data is {get_shape(new_output)}')\n",
    "print()\n",
    "print(f'Shape of input after transposing the data is {get_shape(new_input)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def79264",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nodes, Y_nodes = get_nodes(new_input)[0], get_nodes(new_output)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2e1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the neural network where first value represents number of nodes in input layer\n",
    "# last value represents number of nodes in output layer\n",
    "# values in between represent number of nodes in the hidden layer(s)\n",
    "\n",
    "# Setting the hyper parameters.\n",
    "ANN_LAYER = [X_nodes, 10, Y_nodes]\n",
    "ANN_SIZE = get_layer_size(ANN_LAYER)\n",
    "# please activation functions should be 1 size less than the ANN_LAYER\n",
    "ACTIVATION_FUNCTION = [\"relu\", \"sigmoid\"]\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 30000\n",
    "\n",
    "COST_FUNCTION = \"binary_cross_entropy\"\n",
    "\n",
    "print(f'A {ANN_SIZE - 1} layer ANN with 1 output and {ANN_SIZE - 2} Hidden Layers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b56678",
   "metadata": {},
   "source": [
    "START THE BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdd4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "costs, ann_parameters = Build_Multi_ANN(new_input, new_output, ACTIVATION_FUNCTION, \n",
    "                                       ANN_SIZE, COST_FUNCTION, ANN_LAYER, LEARNING_RATE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34689068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the prediction\n",
    "Y_pred = model_prediction(new_input, ann_parameters, ACTIVATION_FUNCTION, ANN_SIZE)\n",
    "Y_pred1_score = get_accuracy_score(Y_pred, new_output)\n",
    "print(Y_pred1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca5e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = confusion_matrix(Y_pred, new_output)\n",
    "conf = conf.astype(int)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd1d8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "confuse_plot(conf, \"train data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681e908",
   "metadata": {},
   "source": [
    "Part 2 of the model building and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfbc75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(feature, new_label,  test_size=0.20, random_state= 123)\n",
    "\n",
    "# standardize \n",
    "new_scaler = StandardScaler().fit(X_train)\n",
    "new_x_train = standardize_features(new_scaler, X_train)\n",
    "new_x = new_x_train.T\n",
    "new_y = Y_train.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1cdb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "costs, ann_parameters = Build_Multi_ANN(new_x, new_y, ACTIVATION_FUNCTION, \n",
    "                                       ANN_SIZE, COST_FUNCTION, ANN_LAYER, LEARNING_RATE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0bdd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_train = model_prediction(new_x, ann_parameters, ACTIVATION_FUNCTION, ANN_SIZE)\n",
    "Y_pred1_score = get_accuracy_score(Y_pred_train, new_y)\n",
    "print(Y_pred1_score)\n",
    "\n",
    "new_x_test = standardize_features(new_scaler, X_test)\n",
    "new_x2 = new_x_test.T\n",
    "new_y2 = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88031805",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_test = model_prediction(new_x2, ann_parameters, ACTIVATION_FUNCTION, ANN_SIZE)\n",
    "Y_pred2_score = get_accuracy_score(Y_pred_test, new_y2)\n",
    "print(Y_pred2_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dd2e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
